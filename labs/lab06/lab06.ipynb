{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wstęp do Uczenia Maszynowego \n",
    "##### Laboratorium 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, roc_curve\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresja logistyczna z regularyzacją Elastic Net\n",
    "\n",
    "Regresja logistyczna estymuje parametry $\\boldsymbol{\\beta}$ poprzez minimalizację funkcji kosztu (ujemnej log-likelihood) z dodaną karą regularyzacyjną:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\beta}) \n",
    "= -\\frac{1}{n} \\sum_{i=1}^{n} \n",
    "\\Big[ \n",
    "y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \n",
    "\\Big]\n",
    "+ \\lambda \\left(\n",
    "\\alpha \\|\\boldsymbol{\\beta}\\|_1 + \n",
    "\\frac{1 - \\alpha}{2} \\|\\boldsymbol{\\beta}\\|_2^2\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "gdzie:\n",
    "\n",
    "- $p_i = \\frac{1}{1 + e^{-(\\beta_0 + \\mathbf{x}_i^\\top \\boldsymbol{\\beta})}}$  — to prawdopodobieństwo przynależności do klasy pozytywnej,  \n",
    "- $\\lambda > 0$ — współczynnik regularyzacji (im większy, tym silniejsza kara),  \n",
    "- $\\alpha \\in [0, 1]$ — współczynnik mieszający między karą $L_1$ i $L_2$:\n",
    "  - dla $\\alpha = 1$ → **Lasso (L1)**,\n",
    "  - dla $\\alpha = 0$ → **Ridge (L2)**,\n",
    "  - dla $0 < \\alpha < 1$ → **Elastic Net** (mieszanka obu).\n",
    "\n",
    "---\n",
    "W implementacji `scikit-learn` parametr `C` jest odwrotnością $\\lambda$:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{\\lambda},\n",
    "$$\n",
    "a parametrowi $\\alpha$ odpowiada `l1_ratio`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##### *Zadanie 0*\n",
    "-----\n",
    "   Dopasuj modele regresji logistycznej z regularyzacją L1 (penalty='l1') dla wartości  \n",
    "     $$\n",
    "     C \\in \\{0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10, 50, 100\\}.\n",
    "     $$\n",
    "   - Dla każdej wartości `C` oblicz:\n",
    "     - wartość wskaźnika AUC na zbiorze testowym,  \n",
    "     - liczbę niezerowych współczynników modelu.\n",
    "   - Na podstawie wyników wybierz wartość `C`, która stanowi najlepszy kompromis między jakością predykcji a prostotą modelu.\n",
    "\n",
    "\n",
    "   Następnie używając wybranego najlepszego `C`, dopasuj modele regresji logistycznej z regularyzacją Elastic Net, zmieniając parametr  \n",
    "     $$\n",
    "     \\alpha \\in \\{0.0, 0.1, 0.25, 0.5, 0.75, 0.9,  1.0\\}.\n",
    "     $$\n",
    "   - Dla każdej wartości $\\alpha$ oblicz:\n",
    "     - wartość AUC na zbiorze testowym,  \n",
    "     - liczbę niezerowych współczynników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = fetch_openml(name='credit-g', version=1, as_frame=True)\n",
    "df = credit.frame\n",
    "\n",
    "# Zmienna celu: 1 = good, 0 = bad\n",
    "df['class'] = (df['class'] == 'good').astype(int)\n",
    "\n",
    "# One-hot encoding zmiennych kategorycznych\n",
    "X = pd.get_dummies(df.drop(columns='class'), drop_first=True)\n",
    "y = df['class']\n",
    "\n",
    "# Podział na zbiory treningowy/testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standaryzacja danych\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Optymalizacja parametru C dla regularyzacji L1\n",
    "\n",
    "C_values = np.logspace(-3, 2, 10)  # od 0.001 do 100\n",
    "results_L1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Uwaga\n",
    "\n",
    "Wartość optymalna parametru `C` uzyskana dla regularyzacji **L1** może stanowić **dobry punkt wyjścia** do dalszej optymalizacji w modelu **Elastic Net**.  \n",
    "Jednak ze względu na obecność składnika L2 w funkcji kosztu, **optymalne wartości `C` mogą się różnić** między L1 i Elastic Net.  \n",
    "W praktyce warto:\n",
    "- przeprowadzić osobną optymalizację obu hiperparametrów (`C`, `α`),  \n",
    "- lub rozważyć **otoczenie wartości `C_max`** wyznaczonego dla L1, aby sprawdzić stabilność rozwiązania.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losowanie dwóch zmiennych z rozkładu standardowego normalnego\n",
    "rng = np.random.default_rng(1)\n",
    "X = rng.standard_normal((50,2))\n",
    "y = np.array([-1]*25 + [1]*25)\n",
    "# przesunięcie punktów o wektor [2, 2]\n",
    "X[y == 1] += 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wykres punktów\n",
    "plt.scatter(X[:,0], X[:,1], c = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##### *Zadanie 1*\n",
    "-----\n",
    "Zbuduj model wektorów podpierających wykorzystując zdefinowane X i y jak zbiór treningowy. Ustaw parametr `C` = 10, `kernel` = 'linear'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyznaczanie hiperpłaszczyzny (dla dwóch wymiarów)\n",
    "\n",
    "$<w, x> + b = 0$\n",
    "\n",
    "$w_1x_1 + w_2x_2 + b = 0$\n",
    "\n",
    "$w_2x_2 = -w_1x_1 - b$\n",
    "\n",
    "$x_2 = -\\frac{w_1}{w_2}x_1 - \\frac{b}{w_2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = ..\n",
    "b = ..\n",
    "\n",
    "A = ..\n",
    "B = .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##### *Zadanie 2*\n",
    "-----\n",
    "Wyznacz, które punkty są wektorami podpierającymi i zaznacz je na powyższym wykresie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##### *Zadanie 3*\n",
    "-----\n",
    "Zbuduj model wektorów podpierających wykorzystując zdefinowane X i y jak zbiór treningowy. Ustaw parametr `C` = 0.01, `kernel` = 'linear'. Narysuj płaszczyznę rozdzielającą klasy i wskaż wektory podpierające. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyznaczanie marginesu (dla dwóch wymiarów)\n",
    "\n",
    "$margin_{magnitude} = \\frac{1}{||w||}$\n",
    "\n",
    "$||w|| = \\sqrt{w_1^2 + w_2^2}$\n",
    "\n",
    "$\\hat{w} = \\frac{w}{||w||}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hat = .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin =.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary_points = np.array(list(zip(np.array([-3,3]), A * np.array([-3,3]) + B)))\n",
    "points_of_line_above = decision_boundary_points + w_hat * margin\n",
    "points_of_line_below = decision_boundary_points - w_hat * margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(X[:,0], X[:,1], c = y)\n",
    "ax.plot(np.array([-3,3]), A * np.array([-3,3]) + B)\n",
    "# Blue margin line above\n",
    "plt.plot(points_of_line_above[:, 0], \n",
    "         points_of_line_above[:, 1], \n",
    "         'b--', \n",
    "         linewidth=2)\n",
    "# Green margin line below\n",
    "plt.plot(points_of_line_below[:, 0], \n",
    "         points_of_line_below[:, 1], \n",
    "         'g--',\n",
    "         linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optymalizacja parametru C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as skm\n",
    "kfold = skm.KFold(5,\n",
    "                  random_state=0,\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = skm.GridSearchCV(svm_linear,\n",
    "                        {'C':[0.001, 0.01, 0.1, 1, 5, 10, 100]}, \n",
    "                         refit=True,\n",
    "                         cv=kfold,\n",
    "                         scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podsumowanie, czyli co warto wiedzieć...\n",
    "\n",
    "\n",
    "1. Idea regresji liniowej\n",
    "2. Regresja logistyczna\n",
    "\n",
    "    a) przekształcenie funkcją sigmoid\n",
    "\n",
    "    b) wyliczanie współczynników metodą ML\n",
    "\n",
    "    c) regularyzacja L1 + własności (ograniczenie liczby zmiennych)\n",
    "\n",
    "    d) regularyzacja L2\n",
    "\n",
    "3. SVM (liniowe)\n",
    "\n",
    "    a) wyznaczanie hiperpłaszczyzny, marginesu, wektorów podpierających\n",
    "\n",
    "    b) przypadek danych nieseparowalnych - kara C (analogia do L2)\n",
    "    \n",
    "4. Funkcje straty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
